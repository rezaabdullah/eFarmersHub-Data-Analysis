{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eFarmersHub Data Analysis\n",
    "eFarmersHub data are stored in `gds_database` for data visualization. There are 8 tables:\n",
    "1. Income generating tables: Sale, Machine Rent & Advisory\n",
    "2. Expenditure tables: Purchase, Processing, Expense\n",
    "3. User table: This table stores all the user data\n",
    "\n",
    "The script utilizes `SQLAlchemy` as a database toolkit for CRUD operation while `Pandas` is used for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "# data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# database toolkit\n",
    "from sqlalchemy import create_engine, MetaData, inspect, Table, Column, Integer, String, Date, Numeric, extract\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.sql import select\n",
    "\n",
    "# read env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# path handling\n",
    "from pathlib import Path\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "\n",
    "# z-score for anomaly detection\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env variables\n",
    "dotenv_path = Path(\"./.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "HOST = os.getenv(\"HOST\")\n",
    "PORT = os.getenv(\"PORT\")\n",
    "DATABASE = os.getenv(\"DATABASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Sale Table\n",
    "Sale data are stored in `gds_sale_transactions` table. For financial analysis such as revenue and profit, `net_amount` and `cogs_amount` are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sale(engine):\n",
    "    \"\"\"\n",
    "    read gds_sale_transactions table from sql database and returns df\n",
    "    :param engine: SQLAlchemy engine object\n",
    "    :return df: sale dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = \"\"\"\n",
    "                SELECT country_name, parent_name, user_type, user_region, user_name, user_id, market_type,\n",
    "                    business_category, category, product, transaction_date, transaction_id, customer_id, customer_name,\n",
    "                    customer_mobile, customer_gender, product_amount, net_amount, due_amount, cogs_amount, version,\n",
    "                    currency_exchange_rate\n",
    "                FROM (\n",
    "                    SELECT distinct user_id, country_name, user_name, user_type, parent_name, transaction_id,\n",
    "                        transaction_date, customer_id, customer_name, customer_mobile, customer_gender,\n",
    "                        market_type, sale_type, business_category, category_id, category, product_id, product,\n",
    "                        unit_type, attribute, quantity, unit_price, product_amount, sub_total_amount,\n",
    "                        commission_amount, discount_amount, net_amount, paid_amount, due_amount, due_receivable_date,\n",
    "                        version, user_join_date, user_region, customer_join_date, currency_exchange_rate, cogs_amount\n",
    "                    FROM gds_database.gds_sale_transactions\n",
    "                ) sale;\n",
    "                \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename=\"./log\", filemode=\"a\", format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            level=logging.ERROR)\n",
    "        logging.error(e)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sale(df):\n",
    "    \"\"\"\n",
    "    transform sale dataframe and returns df\n",
    "    :param df: actual sale dataframe\n",
    "    :return df: transformed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    # convert date_of_transaction to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    # convert user_id to string\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str)\n",
    "    df[\"customer_mobile\"] = df[\"customer_mobile\"].astype(str)\n",
    "    df[\"transaction_id\"] = df[\"transaction_id\"].astype(str)\n",
    "\n",
    "    # convert and round numerical columns\n",
    "    df[\"product_amount\"] = df[\"product_amount\"].astype(float)\n",
    "    df[\"net_amount\"] = df[\"net_amount\"].astype(float)\n",
    "    df[\"cogs_amount\"] = df[\"cogs_amount\"].astype(float)\n",
    "    df[\"due_amount\"] = df[\"due_amount\"].astype(float)\n",
    "    df[\"currency_exchange_rate\"] = df[\"currency_exchange_rate\"].astype(float)\n",
    "\n",
    "    # renaming columns for consistency\n",
    "    df.rename(columns={\"net_amount\" : \"revenue\"}, inplace=True)\n",
    "\n",
    "    # add transaction_category column to identify the module used for transaction\n",
    "    df[\"transaction_category\"] = \"Sale\"\n",
    "\n",
    "    # sorting data based on version and keep the latest version only\n",
    "    df = df.sort_values([\"country_name\", \"parent_name\", \"user_id\", \"transaction_id\", \"category\", \"product\",\n",
    "            \"version\"]) \\\n",
    "        .drop_duplicates(subset=[\"transaction_id\", \"category\", \"product\"], keep=\"last\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Machine Rent\n",
    "Machine Rent data are stored in `gds_machine_rent_transactions` table. For financial analysis such as revenue and profit, `net_amount` is considered.\n",
    "\n",
    "**Note:** Depreciation is not being considered at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_machine_rent(engine):\n",
    "    \"\"\"\n",
    "    read gds_machine_rent_transactions table from sql database and returns df\n",
    "    :param engine: SQLAlchemy engine object\n",
    "    :return df: sale dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = \"\"\"\n",
    "                SELECT country_name, parent_name, user_type, user_region, user_name, user_id, business_category,\n",
    "                    category, product, transaction_date, transaction_id, customer_id, customer_name,\n",
    "                    customer_mobile, customer_gender, amount, net_amount, due_amount, version, currency_exchange_rate\n",
    "                FROM (\n",
    "                    SELECT distinct user_id, country_name, user_name, user_type, parent_name, transaction_id,\n",
    "                        transaction_date, customer_id, customer_name, customer_mobile, customer_gender,\n",
    "                        business_category, category_id, category, product_id, product, unit_type, quantity,\n",
    "                        unit_price, unit_count, amount, sub_total_amount, net_amount, paid_amount, due_amount,\n",
    "                        due_receivable_date, land_type, land_size, start_date_time, end_date_time, rent_hour,\n",
    "                        version, user_join_date, user_region, customer_join_date, currency_exchange_rate\n",
    "                    FROM gds_database.gds_machine_rent_transactions\n",
    "                ) machine_rent;\n",
    "                \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename=\"./log\", filemode=\"a\", format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            level=logging.ERROR)\n",
    "        logging.error(e)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_machine_rent(df):\n",
    "    \"\"\"\n",
    "    transform machine_rent dataframe and returns df\n",
    "    :param df: actual machine_rent dataframe\n",
    "    :return df: transformed dataframe\n",
    "    \"\"\"\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "    # convert date_of_transaction to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    # convert user_id to string\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str)\n",
    "    df[\"customer_mobile\"] = df[\"customer_mobile\"].astype(str)\n",
    "    df[\"transaction_id\"] = df[\"transaction_id\"].astype(str)\n",
    "\n",
    "    # convert and round numerical columns\n",
    "    df[\"amount\"] = df[\"amount\"].astype(float)\n",
    "    df[\"net_amount\"] = df[\"net_amount\"].astype(float)\n",
    "    df[\"due_amount\"] = df[\"due_amount\"].astype(float)\n",
    "    df[\"currency_exchange_rate\"] = df[\"currency_exchange_rate\"].astype(float)\n",
    "    \n",
    "    # renaming columns for consistency\n",
    "    df.rename(columns={\"net_amount\" : \"revenue\",\n",
    "        \"amount\": \"product_amount\"}, inplace=True)\n",
    "\n",
    "    # add market_type column\n",
    "    df[\"market_type\"] = \"Farmer\"\n",
    "    df[\"transaction_category\"] = \"Machinery Rental\"\n",
    "\n",
    "    # sorting data based on version and keep the latest version only\n",
    "    df = df.sort_values([\"country_name\", \"parent_name\", \"user_id\", \"transaction_id\", \"category\", \"product\",\n",
    "            \"version\"]) \\\n",
    "        .drop_duplicates(subset=[\"transaction_id\", \"category\", \"product\"], keep=\"last\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Advisory Service\n",
    "Advisory data are stored in `gds_advisory_transactions` table. For financial analysis such as revenue and profit, `amount` is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advisory(engine):\n",
    "    \"\"\"\n",
    "    read advisory table from sql database and returns df\n",
    "    :param engine: SQLAlchemy engine object\n",
    "    :return df: sale dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = \"\"\"\n",
    "                SELECT country_name, parent_name, user_type, user_region, user_name, user_id, business_categories,\n",
    "                    categories, transaction_date, transaction_id, customer_id, customer_name, customer_mobile,\n",
    "                        customer_gender, amount, version, currency_exchange_rate\n",
    "                FROM (\n",
    "                    SELECT distinct user_id, country_name, user_name, user_type, user_join_date, user_region,\n",
    "                        parent_name, transaction_id, transaction_date, customer_id, customer_name, customer_mobile,\n",
    "                        customer_gender, customer_join_date, categories_ids, business_categories, categories,\n",
    "                        tags_ids, tags, comments, amount, usd_amount, version, currency_exchange_rate\n",
    "                    FROM gds_database.gds_advisory_transactions\n",
    "                ) advisory;\n",
    "                \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename=\"./log\", filemode=\"a\", format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            level=logging.ERROR)\n",
    "        logging.error(e)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_advisory(df):\n",
    "    \"\"\"\n",
    "    transform advisory dataframe and returns df\n",
    "    :param df: actual machine_rent dataframe\n",
    "    :return df: transformed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "    # convert date_of_transaction to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    # convert user_id to string\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str)\n",
    "    df[\"customer_mobile\"] = df[\"customer_mobile\"].astype(str)\n",
    "    df[\"transaction_id\"] = df[\"transaction_id\"].astype(str)\n",
    "\n",
    "    # convert and round numerical columns\n",
    "    df[\"amount\"] = df[\"amount\"].astype(float)\n",
    "    df[\"currency_exchange_rate\"] = df[\"currency_exchange_rate\"].astype(float)\n",
    "    \n",
    "    # renaming columns for consistency\n",
    "    df.rename(columns={\"amount\" : \"revenue\"}, inplace=True)\n",
    "\n",
    "    # add market_type column\n",
    "    df[\"market_type\"] = \"Farmer\"\n",
    "    df[\"transaction_category\"] = \"Advisory\"\n",
    "    df[\"business_category\"] = \"Advisory\"\n",
    "    df[\"category\"] = \"Advisory\"\n",
    "\n",
    "    # sorting data based on version and keep the latest version only\n",
    "    df = df.sort_values([\"country_name\", \"parent_name\", \"user_id\", \"transaction_id\", \"category\",\n",
    "        \"version\"]) \\\n",
    "        .drop_duplicates(subset=[\"transaction_id\", \"category\"], keep=\"last\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Purchase\n",
    "Purchase data are stored in `gds_purchase_transactions` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_purchase(engine):\n",
    "    \"\"\"\n",
    "    read purchase table from sql database and returns df\n",
    "    :param engine: SQLAlchemy engine object\n",
    "    :return df: sale dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = \"\"\"\n",
    "                SELECT country_name, parent_name, user_type, user_region, user_name, user_id, market_type,\n",
    "                    business_category, category, product, transaction_date, transaction_id, supplier_id, supplier_name,\n",
    "                    supplier_mobile, supplier_gender, product_amount, net_amount, due_amount, version,\n",
    "                    currency_exchange_rate\n",
    "                FROM (\n",
    "                    SELECT distinct user_id, country_name, user_name, user_type, parent_name, transaction_id,\n",
    "                        transaction_date, supplier_id, supplier_name, supplier_mobile, supplier_gender, market_type,\n",
    "                        business_category, category_id, category, product_id, product, unit_type, attribute, quantity,\n",
    "                        unit_price, product_amount, sub_total_amount, commission_amount, net_amount, paid_amount,\n",
    "                        due_amount, due_payable_date, version, user_join_date, user_region, supplier_join_date,\n",
    "                        currency_exchange_rate\n",
    "                    FROM gds_database.gds_purchase_transactions\n",
    "                ) purchase;\n",
    "                \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename=\"./log\", filemode=\"a\", format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.ERROR)\n",
    "        logging.error(e)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_purchase(df):\n",
    "    \"\"\"\n",
    "    transform purchase dataframe and returns df\n",
    "    :param df: actual purchase dataframe\n",
    "    :return df: transformed dataframe\n",
    "    \"\"\"\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    # convert transaction_date to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    # convert user_id to string\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"supplier_id\"] = df[\"supplier_id\"].astype(str)\n",
    "    df[\"supplier_mobile\"] = df[\"supplier_mobile\"].astype(str)\n",
    "    df[\"transaction_id\"] = df[\"transaction_id\"].astype(str)\n",
    "\n",
    "    # convert and round numerical columns\n",
    "    df[\"net_amount\"] = df[\"net_amount\"].astype(float)\n",
    "    df[\"due_amount\"] = df[\"due_amount\"].astype(float)   \n",
    "    df[\"currency_exchange_rate\"] = df[\"currency_exchange_rate\"].astype(float)\n",
    "\n",
    "    # renaming columns for consistency\n",
    "    df.rename(columns={\"net_amount\" : \"expenditure\",\n",
    "        \"product_amount\" : \"product_expenditure\"}, inplace=True)\n",
    "\n",
    "    # add market_type column\n",
    "    df[\"transaction_category\"] = \"Purchase\"\n",
    "\n",
    "    # sorting data based on version and keep the latest version only\n",
    "    df = df.sort_values([\"country_name\", \"parent_name\", \"user_id\", \"transaction_id\", \"category\", \"product\",\n",
    "            \"version\"]) \\\n",
    "        .drop_duplicates(subset=[\"transaction_id\", \"category\", \"product\"], keep=\"last\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Processing\n",
    "Processing data are stored in `gds_processing_transactions` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_processing(engine):\n",
    "    \"\"\"\n",
    "    read processing table from sql database and returns df\n",
    "    :param engine: SQLAlchemy engine object\n",
    "    :return df: sale dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = \"\"\"\n",
    "                SELECT country_name, parent_name, user_type, user_region, user_name, user_id, business_category,\n",
    "                    category, product, transaction_date, transaction_id, amount, production_cost, version,\n",
    "                    currency_exchange_rate\n",
    "                FROM (\n",
    "                    SELECT distinct user_id, country_name, user_name, user_type, parent_name, transaction_id,\n",
    "                        transaction_date, business_category, category_id, category, product_id, product, unit_type,\n",
    "                        quantity, unit_price, amount, production_cost, version, user_join_date, user_region,\n",
    "                        currency_exchange_rate\n",
    "                    FROM gds_database.gds_processing_transactions\n",
    "                ) processing;\n",
    "                \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename=\"./log\", filemode=\"a\", format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.ERROR)\n",
    "        logging.error(e)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_processing(df):\n",
    "    \"\"\"\n",
    "    transform processing dataframe and returns df\n",
    "    :param df: actual processing dataframe\n",
    "    :return df: transformed dataframe\n",
    "    \"\"\"\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    # convert transaction_date to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    # convert user_id to string\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"transaction_id\"] = df[\"transaction_id\"].astype(str)\n",
    "\n",
    "    # convert and round numerical columns\n",
    "    df[\"amount\"] = df[\"amount\"].astype(float)\n",
    "    df[\"production_cost\"] = df[\"production_cost\"].astype(float)\n",
    "    df[\"currency_exchange_rate\"] = df[\"currency_exchange_rate\"].astype(float)\n",
    "\n",
    "    # renaming columns for consistency\n",
    "    df.rename(columns={\"production_cost\" : \"expenditure\",\n",
    "        \"amount\" : \"product_expenditure\"}, inplace=True)\n",
    "\n",
    "    # add transaction category column\n",
    "    df[\"market_type\"] = df[\"user_type\"]\n",
    "    df[\"transaction_category\"] = \"Processing\"\n",
    "\n",
    "    # sorting data based on version and keep the latest version only\n",
    "    df = df.sort_values([\"country_name\", \"parent_name\", \"user_id\", \"transaction_id\", \"category\", \"product\",\n",
    "            \"version\"]) \\\n",
    "        .drop_duplicates(subset=[\"transaction_id\", \"category\", \"product\"], keep=\"last\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Machine Purchase\n",
    "Machine Purchase data are stored in `gds_machine_purchase_transactions` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_machine_purchase(engine):\n",
    "    \"\"\"\n",
    "    read machine_purchase table from sql database and returns df\n",
    "    :param engine: SQLAlchemy engine object\n",
    "    :return df: sale dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = \"\"\"\n",
    "                SELECT country_name, parent_name, user_type, user_region, user_name, user_id, business_category,\n",
    "                    category, product, transaction_date, transaction_id, supplier_id, supplier_name,\n",
    "                    supplier_mobile, supplier_gender, total_amount, due_amount, version,\n",
    "                    currency_exchange_rate\n",
    "                FROM (\n",
    "                    SELECT distinct user_id, country_name, user_name, user_type, parent_name, transaction_id,\n",
    "                        transaction_date, supplier_id, supplier_name, supplier_mobile, supplier_gender,\n",
    "                        business_category, category_id, category, product_id, product, quantity, unit_price,\n",
    "                        total_amount, paid_amount, due_amount, due_payable_date, version, user_join_date, user_region,\n",
    "                        supplier_join_date, currency_exchange_rate\n",
    "                    FROM gds_database.gds_machine_purchase_transactions\n",
    "                ) machine_purchase;\n",
    "                \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename=\"./log\", filemode=\"a\", format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            level=logging.ERROR)\n",
    "        logging.error(e)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_machine_purchase(df):\n",
    "    \"\"\"\n",
    "    transform machine_purchase dataframe and returns df\n",
    "    :param df: actual machine_purchase dataframe\n",
    "    :return df: transformed dataframe\n",
    "    \"\"\"\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    # convert transaction_date to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    # convert user_id to string\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"supplier_id\"] = df[\"supplier_id\"].astype(str)\n",
    "    df[\"supplier_mobile\"] = df[\"supplier_mobile\"].astype(str)\n",
    "    df[\"transaction_id\"] = df[\"transaction_id\"].astype(str)\n",
    "\n",
    "    # convert and round numerical columns\n",
    "    df[\"total_amount\"] = df[\"total_amount\"].astype(float)\n",
    "    df[\"due_amount\"] = df[\"due_amount\"].astype(float)\n",
    "    df[\"currency_exchange_rate\"] = df[\"currency_exchange_rate\"].astype(float)    \n",
    "\n",
    "    # usd conversion\n",
    "    df.rename(columns={\"total_amount\" : \"expenditure\"}, inplace=True)\n",
    "\n",
    "    # add transaction category column\n",
    "    df[\"market_type\"] = df[\"user_type\"]\n",
    "    df[\"transaction_category\"] = \"Machinery Purchase\"\n",
    "\n",
    "    # sorting data based on version and keep the latest version only\n",
    "    df = df.sort_values([\"country_name\", \"parent_name\", \"user_id\", \"transaction_id\", \"category\", \"product\",\n",
    "            \"version\"]) \\\n",
    "        .drop_duplicates(subset=[\"transaction_id\"], keep=\"last\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Expense\n",
    "Expense data are stored in `gds_expense_transactions` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_expense(engine):\n",
    "    \"\"\"\n",
    "    read expense table from sql database and returns df\n",
    "    :param engine: SQLAlchemy engine object\n",
    "    :return df: sale dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = \"\"\"\n",
    "                SELECT country_name, parent_name, user_type, user_region, user_name, user_id, expense_category,\n",
    "                    business_category, product_category, expense_type, transaction_date, transaction_id, total_amount,\n",
    "                    version, currency_exchange_rate\n",
    "                FROM (\n",
    "                    SELECT distinct user_id, country_name, user_name, user_type, parent_name, transaction_id,\n",
    "                        transaction_date, expense_category, expense_type, total_amount, business_category,\n",
    "                        product_category_id, product_category, version, user_join_date, user_region,\n",
    "                        currency_exchange_rate\n",
    "                    FROM gds_database.gds_expense_transactions\n",
    "                ) expense;\n",
    "                \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename=\"./log\", filemode=\"a\", format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            level=logging.ERROR)\n",
    "        logging.error(e)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_expense(df):\n",
    "    \"\"\"\n",
    "    transform expense dataframe and returns df\n",
    "    :param df: actual expense dataframe\n",
    "    :return df: transformed dataframe\n",
    "    \"\"\"\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    # convert transaction_date to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    # convert user_id to string\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"transaction_id\"] = df[\"transaction_id\"].astype(str)\n",
    "\n",
    "    # convert and round numerical columns\n",
    "    df[\"total_amount\"] = df[\"total_amount\"].astype(float)\n",
    "    df[\"currency_exchange_rate\"] = df[\"currency_exchange_rate\"].astype(float)\n",
    "\n",
    "    # revenue & profit\n",
    "    df.rename(columns={\"total_amount\" : \"expenditure\",\n",
    "        \"product_category\" : \"category\"}, inplace=True)\n",
    "\n",
    "    # add transaction category column\n",
    "    df[\"market_type\"] = df[\"user_type\"]\n",
    "    df[\"transaction_category\"] = \"Expense\"\n",
    "\n",
    "    # sorting data based on version and keep the latest version only\n",
    "    df = df.sort_values([\"country_name\", \"parent_name\", \"user_id\", \"transaction_id\", \"expense_category\",\n",
    "            \"category\", \"expense_type\", \"version\"]) \\\n",
    "        .drop_duplicates(subset=[\"transaction_id\", \"expense_category\", \"category\", \"expense_type\"],\n",
    "            keep=\"last\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Identify anomaly for income\n",
    "```python\n",
    "def anomaly_income(df):\n",
    "    \"\"\"\n",
    "    identify data anomaly especially net_amount\n",
    "    :param df: transaction dataframe\n",
    "    :return df: filtered df\n",
    "    \"\"\"\n",
    "    agg_df = df.groupby([\"transaction_category\", \"user_name\", \"user_id\", \"transaction_id\"]) \\\n",
    "        .agg(total_revenue=(\"revenue\", \"sum\")).reset_index()\n",
    "    agg_df[\"z_score\"] = agg_df.groupby([\"user_id\", \"transaction_category\"])[\"total_revenue\"] \\\n",
    "        .transform(lambda x : zscore(x, ddof=1))\n",
    "    anomaly = agg_df.loc[abs(agg_df[\"z_score\"]) > 3]\n",
    "    transaction_id_list = anomaly[\"transaction_id\"].tolist()\n",
    "    mask = df[\"transaction_id\"].isin(transaction_id_list)\n",
    "    df = df[~mask]\n",
    "\n",
    "    return df, anomaly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Identify anomaly for expense\n",
    "```python\n",
    "def anomaly_expenditure(df):\n",
    "    \"\"\"\n",
    "    identify data anomaly especially net_amount\n",
    "    :param df: transaction dataframe\n",
    "    :return df: filtered df\n",
    "    \"\"\"\n",
    "    agg_df = df.groupby([\"transaction_category\", \"user_name\", \"user_id\", \"transaction_id\"]) \\\n",
    "        .agg(total_expenditure=(\"expenditure\", \"sum\")).reset_index()\n",
    "    agg_df[\"z_score\"] = agg_df.groupby([\"user_id\", \"transaction_category\"])[\"total_expenditure\"] \\\n",
    "        .transform(lambda x : zscore(x, ddof=1))\n",
    "    anomaly = agg_df.loc[abs(agg_df[\"z_score\"]) > 3]\n",
    "    transaction_id_list = anomaly[\"transaction_id\"].tolist()\n",
    "    mask = df[\"transaction_id\"].isin(transaction_id_list)\n",
    "    df = df[~mask]\n",
    "\n",
    "    return df, anomaly\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_denormalize(sale, machine_rent, advisory, purchase, machine_purchase, processing, expense):\n",
    "    \"\"\"\n",
    "    denormalize all the transactions i.e. concatanate or compile all the transactional tables into one\n",
    "    master table\n",
    "    :param sale: sale dataframe\n",
    "    :param machine_rent: machine_rent dataframe\n",
    "    :param advisory: advisory dataframe\n",
    "    :param purchase: purchase dataframe\n",
    "    :param machine_purchase: machine_purchase dataframe\n",
    "    :param processing: processing dataframe\n",
    "    :param expense: expense dataframe\n",
    "    :return df: return master data\n",
    "    \"\"\"\n",
    "    # concat all cash-in transactions\n",
    "    income = pd.concat([sale, machine_rent, advisory], sort=False, ignore_index=True)\n",
    "    # income.to_csv(\"../output/income_actual.csv\", index=False)\n",
    "    # income, income_anomaly = anomaly_income(income)\n",
    "    # usd conversion\n",
    "    income[\"product_amount_usd\"] = round(income[\"product_amount\"] / income[\"currency_exchange_rate\"], 4)\n",
    "    income[\"revenue_usd\"] = round(income[\"revenue\"] / income[\"currency_exchange_rate\"], 4)\n",
    "    income[\"cogs_amount_usd\"] = round(income[\"cogs_amount\"] / income[\"currency_exchange_rate\"], 4)\n",
    "    # data cleaning for outliers\n",
    "    anomaly_income = income.loc[(income[\"revenue_usd\"] >= 10000) & (income[\"cogs_amount_usd\"] >= 10000)] \n",
    "    income = income.loc[(income[\"revenue_usd\"] < 10000) & (income[\"cogs_amount_usd\"] < 10000)]\n",
    "    # income.to_csv(\"../output/income_filter.csv\", index=False)\n",
    "\n",
    "    # concat all cash-out transactions\n",
    "    expenditure = pd.concat([purchase, machine_purchase, processing], sort=False, ignore_index=True)\n",
    "    # expenditure.to_csv(\"../output/expenditure_actual.csv\", index=False)\n",
    "    # expenditure, expenditure_anomaly = anomaly_expenditure(expenditure)\n",
    "    # usd conversion\n",
    "    expenditure[\"product_expenditure_usd\"] = round(expenditure[\"product_expenditure\"] / \n",
    "        expenditure[\"currency_exchange_rate\"], 4)\n",
    "    expenditure[\"expenditure_usd\"] = round(expenditure[\"expenditure\"] / expenditure[\"currency_exchange_rate\"], 4)\n",
    "    # data cleaning for outliers\n",
    "    anomaly_expenditure = expenditure.loc[(expenditure[\"expenditure_usd\"] >= 10000)]\n",
    "    expenditure = expenditure.loc[expenditure[\"expenditure_usd\"] < 10000]\n",
    "    # expenditure.to_csv(\"../output/expenditure_filter.csv\", index=False)\n",
    "\n",
    "    # filter direct cost\n",
    "    direct_cost = expense.loc[expense[\"expense_category\"] == \"Direct Cost\"]\n",
    "    # direct_cost.to_csv(\"../output/direct_cost_actual.csv\", index=False)\n",
    "    # direct_cost, direct_cost_anomaly = anomaly_expenditure(direct_cost)\n",
    "    # usd conversion\n",
    "    direct_cost[\"direct_cost_usd\"] = round(direct_cost[\"expenditure\"] / direct_cost[\"currency_exchange_rate\"], 4)\n",
    "    # data cleaning for outliers\n",
    "    anomaly_direct_cost = direct_cost.loc[(direct_cost[\"direct_cost_usd\"] >= 10000)]\n",
    "    direct_cost = direct_cost.loc[direct_cost[\"direct_cost_usd\"] < 10000]\n",
    "    # direct_cost.to_csv(\"../output/direct_cost_filter.csv\", index=False)\n",
    "\n",
    "    # filter indirect cost\n",
    "    indirect_cost = expense.loc[expense[\"expense_category\"] == \"Indirect Cost\"]\n",
    "    # indirect_cost.to_csv(\"../output/indirect_cost_actual.csv\", index=False)\n",
    "    # indirect_cost, indirect_cost_anomaly = anomaly_expenditure(indirect_cost)\n",
    "    # usd conversion\n",
    "    indirect_cost[\"indirect_cost_usd\"] = round(indirect_cost[\"expenditure\"] / indirect_cost[\"currency_exchange_rate\"], 4)\n",
    "    # data cleaning for outliers\n",
    "    anomaly_indirect_cost = indirect_cost.loc[(indirect_cost[\"indirect_cost_usd\"] >= 10000)]\n",
    "    indirect_cost = indirect_cost.loc[indirect_cost[\"indirect_cost_usd\"] < 10000]\n",
    "    # indirect_cost.to_csv(\"../output/indirect_cost_filter.csv\", index=False)\n",
    "    expense = pd.concat([direct_cost, indirect_cost], sort=False, ignore_index=True)\n",
    "    expense_anomaly = pd.concat([anomaly_direct_cost, anomaly_indirect_cost], sort=False, ignore_index=True)\n",
    "    expense.drop(columns=[\"expense_type\"], inplace=True)\n",
    "    expense_anomaly.drop(columns=[\"expense_type\"], inplace=True)\n",
    "\n",
    "    df = pd.concat([income, expenditure, expense], sort=False, ignore_index=True)\n",
    "    anomaly = pd.concat([anomaly_income, anomaly_expenditure, expense_anomaly], sort=False, ignore_index=True)\n",
    "    df.drop(columns=[\"business_categories\", \"categories\"], inplace=True)\n",
    "    anomaly.drop(columns=[\"business_categories\", \"categories\"], inplace=True)\n",
    "    \n",
    "    return df, anomaly\n",
    "    # return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finance(df):\n",
    "    \"\"\"\n",
    "    from master_data table generate finance table\n",
    "    :param df: master_data dataframe\n",
    "    :return df: finance dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.loc[:, [\"country_name\", \"parent_name\", \"user_type\", \"user_region\", \"user_name\", \"user_id\",\n",
    "        \"business_category\", \"category\", \"transaction_date\", \"transaction_id\", \"transaction_category\",\n",
    "        \"revenue_usd\", \"cogs_amount_usd\", \"expenditure_usd\", \"direct_cost_usd\", \"indirect_cost_usd\"]]\n",
    "\n",
    "    # convert transaction_date to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "    \n",
    "    df = df.groupby([\"country_name\", \"user_type\", \"parent_name\", \"user_region\", \"user_name\", \"user_id\",\n",
    "        \"transaction_category\", \"business_category\", \"category\", \"transaction_date\"]) \\\n",
    "        .agg(revenue_usd=(\"revenue_usd\", \"sum\"),\n",
    "            cogs_amount_usd=(\"cogs_amount_usd\", \"sum\"),\n",
    "            expenditure_usd=(\"expenditure_usd\", \"sum\"),\n",
    "            direct_cost_usd=(\"direct_cost_usd\", \"sum\"),\n",
    "            indirect_cost_usd=(\"indirect_cost_usd\", \"sum\"),\n",
    "            transaction_count=(\"transaction_id\", \"nunique\")).reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(df):\n",
    "    \"\"\"\n",
    "    create user table from master_data\n",
    "    :param df: master_data table\n",
    "    :return df: user dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.loc[:, [\"country_name\", \"parent_name\", \"user_type\", \"user_region\", \"user_name\", \"user_id\",\n",
    "        \"transaction_date\", \"transaction_id\", \"customer_id\", \"customer_mobile\", \"customer_gender\", \"supplier_id\",\n",
    "        \"supplier_mobile\", \"supplier_gender\"]]\n",
    "\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "    df = df.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "    df = df.groupby([\"country_name\", \"parent_name\", \"user_type\", \"user_region\", \"user_name\", \"user_id\",\n",
    "        \"transaction_date\"]) \\\n",
    "        .agg(transaction_count=(\"transaction_id\", \"nunique\"),\n",
    "            mobile_count=(\"customer_mobile\", \"nunique\"),\n",
    "            gender_count=(\"customer_gender\", \"count\"),\n",
    "            customer_count=(\"customer_id\", \"nunique\")).reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_customer(group):\n",
    "    return pd.Series(1, index=group.groupby([\"customer_id\"]).head(1).index).reindex(group.index).fillna(0).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_supplier(group):\n",
    "    return pd.Series(1, index=group.groupby([\"supplier_id\"]).head(1).index).reindex(group.index).fillna(0).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling transactions\n",
      "denormalizing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1321/3790990124.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  direct_cost[\"direct_cost_usd\"] = round(direct_cost[\"expenditure\"] / direct_cost[\"currency_exchange_rate\"], 4)\n",
      "/tmp/ipykernel_1321/3790990124.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indirect_cost[\"indirect_cost_usd\"] = round(indirect_cost[\"expenditure\"] / indirect_cost[\"currency_exchange_rate\"], 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create finance table\n",
      "create user data\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # initiate connection to database\n",
    "    connect_url = URL.create(\n",
    "        \"mysql+pymysql\",\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        database=DATABASE\n",
    "    )\n",
    "    engine = create_engine(connect_url)\n",
    "\n",
    "    # debug\n",
    "    # with engine.connect() as conn:\n",
    "    #     inspector = inspect(engine)\n",
    "    #     table_names = inspector.get_table_names()\n",
    "    #     print(table_names)\n",
    "\n",
    "    print(\"compiling transactions\")\n",
    "    # sale\n",
    "    sale = extract_sale(engine)\n",
    "    sale = transform_sale(sale)\n",
    "    # sale.to_csv(\"../output/sale.csv\", index=False)\n",
    "\n",
    "    # machine rent\n",
    "    machine_rent = extract_machine_rent(engine)\n",
    "    machine_rent = transform_machine_rent(machine_rent)\n",
    "    # machine_rent.to_csv(\"../output/machine_rent.csv\", index=False)\n",
    "\n",
    "    # advisory\n",
    "    advisory = extract_advisory(engine)\n",
    "    advisory = transform_advisory(advisory)\n",
    "    # advisory.to_csv(\"../output/advisory.csv\", index=False)\n",
    "\n",
    "    # purchase\n",
    "    purchase = extract_purchase(engine)\n",
    "    purchase = transform_purchase(purchase)\n",
    "    # purchase.to_csv(\"../output/purchase.csv\", index=False)\n",
    "\n",
    "    # processing\n",
    "    processing = extract_processing(engine)\n",
    "    processing = transform_processing(processing)\n",
    "    # processing.to_csv(\"../output/processing.csv\", index=False)\n",
    "\n",
    "    # machine purchase\n",
    "    machine_purchase = extract_machine_purchase(engine)\n",
    "    machine_purchase = transform_machine_purchase(machine_purchase)\n",
    "    # machine_purchase.to_csv(\"../output/machine_purchase.csv\", index=False)\n",
    "\n",
    "    # expense\n",
    "    expense = extract_expense(engine)\n",
    "    expense = transform_expense(expense)\n",
    "    # expense.to_csv(\"../output/expense.csv\", index=False)\n",
    "\n",
    "    print(\"denormalizing data\")\n",
    "    master_data, anomaly = data_denormalize(sale, machine_rent, advisory, purchase, machine_purchase, processing, expense)\n",
    "    # master_data.to_sql(\"master_data_global\", con=engine, if_exists='replace', index = False)\n",
    "    # master_data.to_csv(\"../output/master_data.csv\", index=False)\n",
    "    user_table = master_data.loc[:, [\"country_name\", \"parent_name\", \"user_type\", \"user_region\", \"user_name\", \"user_id\",\n",
    "        \"currency_exchange_rate\"]]\n",
    "    user_table = user_table.groupby([\"country_name\", \"parent_name\", \"user_type\", \"user_region\", \"user_name\", \"user_id\"]) \\\n",
    "        .agg(currency_exchange_rate=(\"currency_exchange_rate\", \"mean\")).reset_index()\n",
    "    master_data.to_csv(\"../output/master_data.csv\", index=False)\n",
    "    # anomaly.to_csv(\"../output/anomaly.csv\", index=False)\n",
    "    # user_table.head()\n",
    "    # anomaly = user_table.merge(anomaly, how=\"outer\", on=[\"user_name\", \"user_id\"])\n",
    "    # anomaly = anomaly.fillna(0)\n",
    "    # anomaly[\"transaction_amount\"] = anomaly[\"total_revenue\"] + anomaly[\"total_expenditure\"]\n",
    "    # anomaly.drop(columns=[\"total_revenue\", \"total_expenditure\"], inplace=True)\n",
    "    # anomaly.to_sql(\"anomaly_global\", con=engine, if_exists='replace', index = False)\n",
    "\n",
    "    print(\"create finance table\")\n",
    "    # financial table\n",
    "    finance = get_finance(master_data)\n",
    "    # finance.to_sql(\"finance_global\", con=engine, if_exists='replace', index = False)\n",
    "\n",
    "    print(\"create user data\")\n",
    "    client = get_user(master_data)\n",
    "    # client.to_sql(\"client_global\", con=engine, if_exists='replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "381d99941aa843f9feea4709de25f28f27171c4591a40a61ca7894a7d74153c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
